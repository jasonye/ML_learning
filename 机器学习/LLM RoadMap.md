
初探大语言模型（LLM）：从基础储备到项目实践的学习路径

来源：https://zhuanlan.zhihu.com/p/1886373364488196872

大语言模型（Large Language Model，LLM）入门学习路线包括了三个方面：

大语言模型基础: 这涵盖了学习LLM所需的基本知识，包括数学、Python编程语言以及神经网络的原理。
大语言模型前沿算法和框架: 在这一部分，重点是利用最新的技术构建LLM。这包括研究和应用先进的算法和模型架构，以提高模型的性能和效率。
大语言模型工程化: 这一部分专注于创建基于LLM的应用程序，并将这些应用部署到实际环境中。这包括学习如何将LLM集成到各种平台和系统中，以及如何确保这些系统的稳定性和可扩展性。

# 1. 大语言模型基础
包括了数学基础知识、python基础、神经网络和NLP自然语言四个方面的学习。

1.1 机器学习的数学
线性代数：理解算法的关键。主要概念包括向量、矩阵、行列式、特征值和特征向量、向量空间和线性变换。
微积分：许多机器学习算法涉及连续函数的优化，这需要理解导数、积分、极限和级数。多变量微积分和梯度的概念也很重要。
概率与统计：对于理解模型如何从数据中学习也同样重要。主要概念包括概率论、随机变量、概率分布、期望、方差、协方差、相关性、假设检验、置信区间、最大似然估计和贝叶斯推断。
【完整版的大模型 AI 学习资料已经打包好，朋友们如果需要可以点击下方小卡片 100%免费】



1.2 机器学习的Python
Python一直是机器学习和深度学习的首选语言，这得益于其可读性、一致性和鲁棒的数据科学库生态系统。

Python基础：理解基本语法、数据类型、错误处理和面向对象编程。
数据科学库：包括熟悉NumPy进行数值操作，Pandas进行数据操作和分析，Matplotlib和Seaborn进行数据可视化。
数据预处理：涉及特征缩放和规范化、处理缺失数据、异常值检测、分类数据编码以及将数据分割成训练、验证和测试集。
机器学习库：熟练掌握Scikit-learn，这是一个提供了许多监督和非监督学习算法的库。重点需要了解如何实现线性回归、逻辑回归、决策树、随机森林、最近邻(K-NN)和K均值聚类等算法。

1.3. 神经网络
基础知识：这包括理解神经网络的结构，如层、权重、偏置和激活函数（sigmoid、tanh、ReLU等）。
训练和优化：熟悉反向传播和不同类型的损失函数，如均方误差（MSE）和交叉熵。理解各种优化算法，如梯度下降、随机梯度下降、RMSprop和Adam。
过拟合：理解过拟合的概念（模型在训练数据上表现良好但在未见数据上表现差）并学习各种正则化技术（dropout、L1/L2正则化、早停、数据增强）以防止它。
实现多层感知器（MLP）：使用PyTorch构建一个MLP，也称为全连接网络。

1.4. 自然语言处理（NLP）
NLP在许多应用中扮演着关键角色，如翻译、情感分析、聊天机器人等。

文本预处理：学习各种文本预处理步骤，如分词（将文本分割成单词或句子）、词干提取（将单词还原为其根形式）、词形还原（类似于词干提取但考虑上下文）、停用词去除等。
特征提取技术：熟悉将文本数据转换为机器学习算法能理解的格式的技术。关键方法包括词袋模型（BoW）、词频-逆文档频率（TF-IDF）和n-gram。
词嵌入：词嵌入是一种单词表示，允许具有相似含义的单词具有相似的表示。关键方法包括Word2Vec、GloVe和FastText。
循环神经网络（RNNs）：理解RNNs的工作原理，这是一种为序列数据设计的神经网络类型。探索LSTMs和GRUs，两种RNN变体，它们能够学习长期依赖关系。

# 2. 大语言模型前沿算法和框架
2.1. 大语言模型（LLM）架构
需要清楚地了解模型的输入（token）和输出（logits），而原始的注意力机制（ attention mechanism）是另一个必须掌握的关键部分，因为它是很多改进算法的基础，具体来说需要包括以下技术。

高层视角（High-level view:）：编码器encoder-解码器decoder的Transformer架构，特别是仅有解码器的GPT架构，几乎所有流行LLM都应用了该架构。
令牌化（Tokenization）：如何将原始文本数据转换成模型能理解的格式，这包括将文本拆分成Token（通常是单词或子词）。
注意力机制（Attention mechanisms）：掌握注意力机制的理论，包括自注意力和缩放点积注意力，这使得模型能够在产生输出时关注输入的不同部分。
文本生成（Text generation）：模型生成输出序列的多种方式。常见方法包括了贪婪解码（greedy decoding）、束搜索（beam search）、top-k采样（top-k sampling,）和核心采样（nucleus sampling）。
【完整版的大模型 AI 学习资料已经打包好，朋友们如果需要可以点击下方小卡片 100%免费】


### 2.2. 构建指令数据集
虽然从维基百科和其他网站可以轻松地找到原始数据，但何如将数据转换为问题和答案的配对配对却很难。而数据集的质量将直接影响模型的质量，它们是大模型微调（finetune）过程中最重要的组成部分。

Alpaca-样式数据集：使用OpenAI API（GPT）从头开始生成合成数据。你可以指定种子和系统提示以创建多样化的数据集。
高级技术：学习如何通过Evol-Instruct改进现有数据集，如何像在Orca和phi-1论文中那样生成高质量的合成数据。
数据过滤：使用正则表达式、移除近似重复项、关注令牌数较多的答案等传统技术。
提示模板：在没有真正标准的问题和答案的情况下，了解不同的聊天模板很重要，比如ChatML、Alpaca等。
2.3. 模型预训练
是指从大量的无监督数据集中进行模型预训练，模型预训练是一个非常漫长和消费资源的过程，因此它不是本学习路线教程的重点。但是我们可以了解它，以方便后续的学习。它主要包括以下几个部分：

数据管道：预训练需要巨大的数据集（例如，Llama 2是在2万亿令牌上训练的），这些数据集需要被过滤、令牌化，并与预定义的词汇表进行整合。
因果语言模型：了解因果和掩码语言模型的区别，以及在这种情况下使用的损失函数。为了有效的预训练，了解更多关于Megatron-LM或gpt-neox。
规模化定律：规模化定律描述了基于模型大小、数据集大小和用于训练的计算量的预期模型性能。
高性能计算：这里不讨论，但如果你计划从头开始创建自己的LLM（硬件、分布式工作负载等），则需要更多关于HPC的知识是基础。

### 2.4. 监督式微调（Supervised Fine-Tuning）
监督式微调就是让我们在已经标注的数据集上对已经预训练好的模型进行再次训练，以符合任务需求，它是一个非常重要的过程。

完全微调（Full fine-tuning）：完全微调指的是训练模型中的所有参数。这不是一种高效的技术，但它产生稍好的结果。
LoRA：一种基于低秩适配器的参数高效技术（PEFT）。我们只训练这些适配器，而不是所有参数。
QLoRA：另一种基于LoRA的PEFT，它还将模型的权重量化为4位，并引入分页优化器来管理内存峰值。将其与Unsloth结合使用，可以在免费的Colab笔记本上有效运行。
Axolotl：一个用户友好且强大的微调工具，被用于许多最先进的开源模型。
DeepSpeed：高效的预训练和微调LLM，适用于多GPU和多节点设置（在Axolotl中实现）。

### 2.5. 通过人类反馈进行强化学习（Reinforcement Learning from Human Feedback）
在监督式微调之后，RLHF是一个用来将LLM产生的答案达到和人类回答差不多的重要步骤。其思想是从人工反馈中学习偏好。它比SFT更复杂，但是该步骤通常被视为可选的。

偏好数据集（Preference datasets）：这些数据集通常包含几个答案，并有某种排名，但是这种数据集更难产生。
近端策略优化（Proximal Policy Optimization）：这个算法利用一个奖励模型来预测给定文本是否被人类高度评价。然后使用这个预测来优化SFT模型，一般使用基于KL散度加上惩罚的方式来执行。
直接偏好优化（Direct Preference Optimization）：DPO简化将其重新构架为一个分类问题。它使用参考模型而不是奖励模型（无需训练），只需要一个超参数，使其更稳定和高效。

### 2.6. 评估（Evaluation）
评估LLM是一个被低估的部分，它既耗时但是又相对可靠。你的下游任务应该决定你想评估什么，但始终记住Goodhart法则：“当一个指标成为目标时，它就不再是一个好的指标。”

传统指标：困惑度和BLEU分数这样的指标不再像以前那样受欢迎，因为在大多数情况下它们是有缺陷的。了解它们以及何时可以应用它们很重要。
通用基准：基于语言模型评估工具，Open LLM排行榜是通用LLM（如ChatGPT）的主要基准。还有其他流行的基准，如BigBench、MT-Bench等。
特定任务基准：如摘要、翻译和问答等任务有专门的基准、指标甚至子领域（医疗、金融等），如PubMedQA用于生物医学问答。
人类评估：最可靠的评估是用户的接受率或人类做出的比较。如果你想知道模型是否表现良好，最简单但最可靠的方式是自己使用它。
【完整版的大模型 AI 学习资料已经打包好，朋友们如果需要可以点击下方小卡片 100%免费】



### 2.7. 量化
量化是将模型的权重（和激活）使用更低精度进行转换的过程。例如，使用16位存储的权重可以转换为4位表示。这项技术已经越来越重要，因为它可以减少与LLM相关的计算和内存成本，以使其在计算资源更低的设备上运行。

基础技术：了解不同的精度水平（FP32, FP16, INT8等）以及如何使用absmax和零点技术进行朴素量化。
GGUF和llama.cpp：最初设计用于在CPU上运行，llama.cpp和GGUF格式已成为在消费级硬件上运行LLM的最受欢迎的工具。
GPTQ和EXL2：GPTQ特别是EXL2格式提供了惊人的速度，但只能在GPU上运行。模型也需要很长时间才能被量化。
AWQ：这种新格式比GPTQ更准确（困惑度更低），但使用的VRAM更多，不一定更快。

### 2.8. 新趋势
位置嵌入（Positional embeddings）：了解LLM如何编码位置，尤其是RoPE这样的相对位置编码方案。实现YaRN（通过温度因子乘以注意力矩阵）或ALiBi（基于token距离的注意力惩罚）以延长上下文长度。
模型合并（Model merging）：合并训练好的模型已成为创建性能模型而无需任何微调的流行方式。流行的mergekit库实现了最受欢迎的合并方法，如SLERP、DARE和TIES。
专家混合：Mixtral因其出色的性能而重新流行化MoE架构。与此同时，OSS社区出现了一种通过合并模型（如Phixtral）的frankenMoE，这是一个更便宜且性能良好的选项。
多模态模型：这些模型（如CLIP、Stable Diffusion或LLaVA）处理多种类型的输入（文本、图像、音频等），具有统一的嵌入空间，解锁了强大的应用，如文本到图像。

# 3. LLM工程化
在这阶段集中于如何构建和部署基于大语言模型（LLM）的应用程序，以便在生产环境中使用。它分为几个部分，每部分都聚集于LLM应用开发的不同方面：

## 3.1 运行大型语言模型 (LLMs)
运行LLMs可能会因为硬件要求而变得困难。而我们可以通过Api的方式（如GPT-4）来简单的使用大模型。当然也可以进行本地运行。无论哪种方式，都需要额外的提示和引导技巧（也叫做提示工程， prompting engineer）来提升模型的输出质量。

LLM APIs: API是部署LLMs的一种比较简单的方式，它不要求设备拥有显卡资源，但是这种一般需要付费得到API。这个领域分为私有LLMs(OpenAI, Google, Anthropic, Cohere, 等.) 和开源LLMs (OpenRouter, Hugging Face, Together AI, 等.).
开源LLMs:Hugging Face Hub开源了大量的LLMs。你可以直接在Hugging Face Spaces中运行其中一些，或者下载并在像LM Studio这样的应用程序中或通过CLI与llama.cpp或Ollama在本地运行它们。
提示工程（Prompt engineering）：常见技术包括零次提示、少数提示、思维链和ReAct。它们在更大的模型上效果更好，也可以适应更小的模型。
结构化输出（Structuring outputs）：大部分任务需要结构化输出，如严格的模板或JSON格式。可以使用LMQL、Outlines、Guidance等库来指导生成并遵循给定的结构。

## 3.2. 构建向量存储（Building a Vector Storage）
有时候我们想要在特定知识库下让LLMs搜索答案，而检索增强生成（RAG）结合了信息检索（IR）方法的能力，提高文本生成任务的质量和相关性。这种方法在处理需要广泛背景知识或特定信息的任务时特别有用，例如问答、文章撰写、摘要生成等。构建向量存储是构建检索增强生成（RAG）管道的第一步。它涉及文档加载，拆分，生成向量表示（嵌入），并存储等步骤：

文档加载：文档加载器可以处理多种格式：PDF、JSON、HTML、Markdown等。它们还可以直接从一些数据库和API（GitHub、Reddit、Google Drive等）检索数据。
文档拆分：文本拆分器将文档拆分成更小、有语义信息的块。与其在_n_个字符后拆分文本，不如更好地按标题或递归拆分，附加一些额外的元数据。
嵌入模型：嵌入模型将文本转换为向量表示。这对于执行语义搜索至关重要，可以深入并更细致地理解语言。
向量数据库：向量数据库（如Chroma、Pinecone、Milvus、FAISS、Annoy等）旨在存储嵌入向量。它们能够根据向量相似度高效检索与查询“最相似”的数据。

## 3.3. 检索增强生成 (Retrieval Augmented Generation, RAG)
RAG技术可以使LLMs从数据库检索上下文文档以提高其答案的准确性。RAG是一种流行的增强模型知识的方式，无需任何微调。

协调器（Orchestrators）：协调器（如LangChain、LlamaIndex、FastRAG等）是将LLMs与工具、数据库、记忆等连接并增强其能力的流行框架。
检索器（Retrievers）：用户指令并不是为检索优化的。可以应用不同技术（例如，多查询检索器、HyDE等）来重述/扩展它们并提高性能。
记忆（Memory）：为了记住以前的指令和答案，LLMs和聊天机器人（如ChatGPT）将这个历史添加到它们的上下文窗口中。这个缓冲区可以通过摘要（例如，使用一个较小的LLM）、向量存储+RAG等来改进。
评估：我们需要评估文档检索（上下文的精度和召回率）和生成阶段（保真度和答案相关性）。可以使用工具Ragas和DeepEval来简化这一过程。
【完整版的大模型 AI 学习资料已经打包好，朋友们如果需要可以点击下方小卡片 100%免费】


## 3.4. 高级RAG
现实生活中的应用可能需要复杂的管道，包括SQL或图数据库，以及自动选择相关工具和API。这些高级技术可以改进基线解决方案并提供额外功能。

查询构造（Query construction）：存储在传统数据库中的结构化数据需要特定的查询语言，如SQL、Cypher、元数据等。我们可以直接将用户指令翻译成查询以访问数据。
代理和工具（Agents and tools）：代理通过自动选择最相关的工具来增强LLMs，以提供答案。这些工具可以简单到使用Google或Wikipedia，或更复杂，如Python解释器或Jira。
后处理（Post-processing:）：向LLM提供输入的最终步骤。它通过重新排列、RAG-fusion和分类等方式增强检索到的文档的相关性和多样性。

## 3.5. 推理优化 （ Inference optimization）
文本生成是一个成本高昂的过程，需要昂贵的硬件资源。除了量化，还提出了各种技术来最大化吞吐量并降低推理成本。

Flash Attention：优化注意力机制，将其复杂度从二次方降低到线性，加速训练和推理。
键值缓存：了解键值缓存以及多查询注意力（MQA）和分组查询注意力（GQA）中引入的改进。
推测解码：使用小型模型生成草稿，然后由更大的模型审查，以加速文本生成。

## 3.6. 部署LLMs
部署LLMs是一项工程壮举，可能需要多个GPU集群。

本地部署：与私有LLMs相比，开源LLMs可以保护用户隐私，是它的一大优势。本地LLM服务器（LM Studio、Ollama、oobabooga、kobold.cpp等）利用这一优势为本地应用提供动力。
demo部署：Gradio和Streamlit等框架有助于原型化应用程序并分享demo。你也可以轻松地在线托管它们，例如使用Hugging Face Spaces。
服务器部署：在规模上部署LLMs需要云（参见SkyPilot）或本地基础设施，并常常利用优化的文本生成框架，如TGI、vLLM等。
边缘部署：在受限环境中，高性能框架如MLC LLM和mnn-llm可以在网页浏览器、Android和iOS中部署LLM。
7. 保护LLMs
除了与软件相关的传统安全问题外，由于LLMs的训练和提示方式，它们还有独特的弱点。

提示黑客攻击：与提示工程相关的不同技术，包括提示注入（额外指令以劫持模型的答案）、数据/提示泄露（检索其原始数据/提示）和越狱（制作提示以绕过安全功能）。
后门：攻击向量可以针对训练数据本身，通过在训练数据中下毒（例如，使用错误信息）或创建后门（秘密触发器以在推理期间改变模型的行为）。
防御措施：保护你的LLM应用程序的最佳方式是针对这些漏洞测试它们（例如，使用红队测试和像garak这样的检查）并在生产中观察它们（使用框架，如langfuse）。
【完整版的大模型 AI 学习资料已经打包好，朋友们如果需要可以点击下方小卡片 100%免费】