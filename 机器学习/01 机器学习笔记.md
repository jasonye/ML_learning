[toc]

# 数学基础

* 平均数：所有数的平均

* 中位数

* 标准差：标准差定义为[方差](%E6%96%B9%E5%B7%AE.html "方差")的[算术平方根](%E7%AE%97%E6%9C%AF%E5%B9%B3%E6%96%B9%E6%A0%B9.html "算术平方根")，反映组内个体间的离散程度；标准差与[期望值](%E6%9C%9F%E6%9C%9B%E5%80%BC.html "期望值")之比为[标准离差率](%E6%A0%87%E5%87%86%E7%A6%BB%E5%B7%AE%E7%8E%87.html "标准离差率")。

* 方差：数和中位数的差值的平方，然后求平均值

* percentile n. 百分位 adj. 按百等分排列的


# 机器学习库和工具

   参考Python文档学习NumPy、Pandas、Scikit-learn、Matplotlib、TensorFlow等机器学习库的使用

## 机器学习库

### NumPy: 机器学习数学基础库、Python操作数据的基础库。

Many mathematical operations can be accomplished using NumPy, which makes it a popular Python library for multidimensional array and matrix processing.

### Pandas: Python库，进行机器学习数据集处理

笔记见Python_pandas.md 文件
相关文档：
https://pandas.pydata.org/docs/getting_started/index.html

基于NumPy的数据分析工具、

Higher-level data sets are prepared for training and machine learning by using the Python library called pandas, which is another library that is built on NumPy.



### Matplotlib：数据的图形化展示
   笔记参考Python文档

### seaborn:
This Python library utilizes pandas data structures and is built on matplotlib, which focuses on plotting and data visualization.

https://seaborn.pydata.org/tutorial/introduction.html

### scikit-learn: 机器学习算法库

This is a renowned machine learning library based on NumPy and SciPy. It can be used for data mining, simulation, and analysis, supporting most conventional supervised and unsupervised learning techniques.

网站：
参考介绍文档，学习各种使用方法
https://scikit-learn.org/stable/getting_started.html#next-steps
https://scikit-learn.org/stable/modules/compose.html
https://scikit-learn.org/stable/api/index.html

安装：

```shell
pip3 install scikit-learn
```

 概念：
 1. estimator basics：ski-learn中的算法库中的算法都叫estimator
   具体estimator的选择，参考文档：https://scikit-learn.org/stable/machine_learning_map.html#ml-map
 2. Transformer & pre-processors
   对数据进行转换和预处理。比如对缺失的数据进行补齐、离散数据进行数值化等。

 3. Pipeline：
   Transformer和Estimator（Predictors）组合成Pipeline

4. Model evalution 
   通过cross-validation来验证算法的效果。比如 5-fold cross-validation

5. 自动参数搜索
Scikit-learn提供工具自动搜索算法的超参（通过cross-validation），

### keras.datasets
> https://keras.io/api/datasets/

Keras建立在tensorFlow等框架之上。是是出TensorFlow外最流行的深度学习框架。

The keras.datasets module provide a few toy datasets (already-vectorized, in Numpy format) that can be used for debugging a model or creating simple code examples.

If you are looking for larger & more useful ready-to-use datasets, take a look at TensorFlow Datasets.


###  MNIST 数据集--用于测试和练习



机器学习领域的经典数据集，由美国国家标准和技术研究院收集。包含了6w张训练图像和1万张测试图像。
MNIST is a subset of a larger set available from NIST (it's copied from http://yann.lecun.com/exdb/mnist/)

### PyTorch---深度学习框架

PyTorch是一个用于机器学习和深度学习的开源深度学习框架，由Facebook于2016年发布，其主要实现了自动微分功能，并引入动态计算图使模型建立更加灵活。Pytorch可分为前后端两个部分，前端是与用户直接交互的python API，后端是框架内部实现的部分，包括Autograd，它是一个自动微分引擎。

Pytorch基于已有的张量库Torch开发，在PyTorch的早期版本中，使用的是Torch7，后来随着PyTorch的发展，逐渐演变成了PyTorch所使用的张量库。
现如今，Pytorch已经成为开源机器学习系统中，在科研领域市场占有率最高的框架，其在AI顶会上的占比在2022年已达80％ [1]。

参考教程：https://www.runoob.com/pytorch/pytorch-tensor.html

### TensorFlow----深度学习框架
难度比其他机器学习框架更高

基于Python的免费的机器学习开源平台，由Google开源。与NumPy类似，可操作数值张量计算。它是一个平台、拥有庞大的组件生态系统。

A high-level language can generate a function’s derivatives with the help of the open-source Python library, which focuses on differentiable programming.

[TensorFlow](https://www.tensorflow.org/) is a library for developing and training machine learning models.

参考学习教程： https://www.tensorflow.org/tutorials?hl=zh-cn

###  Keras ----- TensorFlow的高阶API 

tf.keras 是用于构建和训练深度学习模型的 TensorFlow 高阶 API。利用此 API，可实现快速原型设计、先进的研究和生产，它具有以下三大优势：

* 方便用户使用 
   Keras 具有针对常见用例做出优化的简单而一致的界面。它可针对用户错误提供切实可行的清晰反馈。
* 模块化和可组合
   将可配置的构造块组合在一起就可以构建 Keras 模型，并且几乎不受限制。
* 易于扩展
   可以编写自定义构造块，表达新的研究创意；并且可以创建新层、指标、损失函数并开发先进的模型。


深度学习API: 机器学习训练数据集等
大多数就深度学习从业者使用。庞大的用户群。

Keras is an API built on top of TensorFlow designed for neural networks and deep learning.


#### XGBoost

#### LightGBM

梯度提升树的用户通常使用这个scikit-learn、LightGBM、XGBoost

主宰Kaggle的3个库：scikit-learn、XGBoost、LightGBM


## 开发工具 

### 1. Jupter  

   机器学习和大模型开发、交流的笔记本工具

   [Project Jupyter | Installing Jupyter](https://jupyter.org/install)

   用来进行机器学习代码的开发、交流、分享等。 Jupyter notebooks 非常适合学习如何使用大型语言模型系统，

   数据预处理工具---Jupyter notebook 开发的使用

   A notebook is a shareable document that combines computer code, plain language descriptions, data, rich visualizations like 3D models, charts, graphs and figures, and interactive controls. A notebook, along with an editor (like JupyterLab), provides a fast interactive environment for prototyping and explaining code, exploring and visualizing data, and sharing ideas with others.

   安装和使用：

   安装办法： [Project Jupyter | Installing Jupyter](https://jupyter.org/install)

   ```shell
   python3 -m venv python_env     // 创建虚拟目录

   // 激活Python开发环境很重要
   source ~/python_env/bin/activate // 激活开发环境，多个不同环境都可以激活

   pip3 install jupyterlab

   jupyter lab  // 启动jupyter编辑器：http://localhost:8888/lab/tree/ex/python/Untitled.ipynb

   // 然后在浏览器里面可以看到、并且执行 http://localhost:8888/lab/tree/ex/LLM/langchain.ipynb
   ```

### 2. Google Colaboratory  
   免费的Jupyter笔记本服务

   Google提供的免费托管服务，实际上是一个网页。

   参考学习: https://colab.research.google.com/

   [我的测试Helloworld用例](https://colab.research.google.com/drive/11wgJFDxFyq7cQn09bC_6WHQiqqgC47BY#scrollTo=TaRgaz8PDb1R)

### 3. Anaconda 流行的机器学习环境、

   本地安装执行的Python编辑环境、


### conda

Conda is an open source package management system and environment management system for installing multiple versions of software packages and their dependencies and switching easily between them. It works on Linux, OS X and Windows, and was created for Python programs but can package and distribute any software.

## 社区

1. Kaggle网站：机器学习竞赛网站
   
   Kaggle比Colab的优势是自带很多数据集、社区氛围浓厚、

   https://www.kaggle.com/code/dansbecker/basic-data-exploration

   Kaggle API的使用、数据集的管理使用等，参考文档：https://www.kaggle.com/docs/api

# 建立深度学习工作区

1. 学习使用Colab、Keras-API、TensorFlow平台的使用。


学习数据的来源：
可以使用Kaggle、openml等平台的数据。
* openml.org is a public repository for machine learning data and experiments, that allows everybody to upload open datasets.


# 机器学习概念

机器学习：常常用来解决两类问题：regression 和 classification 回归和分类问题。

## 机器学习和深度学习

* 机器学习：Machine learning is a subfield of artificial intelligence that focuses on creating models and algorithms that allow computers to learn from and make predictions or decisions on the basis of data.

* 深度学习：Deep learning is an area of machine learning that applies artificial neural networks inspired by the functions of the human brain.


## 机器学习分类

1. 几大类别
   
   通过数据是否有标签等来区分：
   
   * Supervised Learning(监督学习)：需要标注数据标签
   * Unsupervised Learning ：不需要标签数据
   * 半监督学习：介于上述两者之间，部分数据具有标签
   * Reinforcement Learning（强化学习）：智能体如何基于环境而作出行为反应，以取得最大化的累计奖励

   通过解决问题的方法来区别：
   * 深度学习

2. 监督学习的两个应用场景：

   * Regression（回归），主要是预测值
   * Classification（分类），主要是预测类别
   
   回归的应用：比如预测房价、预测商品销量等等
   分类的应用：车牌号的识别、动物分类等、

   无监督学习的应用场景：
   * 聚类、
   * 关联规则：购物篮分析，推荐同样爱好的人还喜欢的其他物品

   强化学习的应用：
   * 聊天机器人、
   * 无人驾驶等

3. 常见的监督学习算法
   * Linear Regression
   * Logistic Regression
   * Decision Trees
   * SVM(Support Vector Machines)
   * k-Nearest Neighbors(k-NN)
   * Naive Bayes
   * Random Forest(Bagging Algorithm)
   * Ensemble Learning(组合)

4. 无监督学习

   三类主要的无监督学习：
   * Clustering
   * Association Rule Mining
   * Dimensionality Reduction

5. 半监督学习

6. 其他概念

   * 特征工程：指对数据特征的整理和优化工作、让它们更易被机器所学习。 


### 有监督学习

**supervised learning** is a type of machine learning where the algorithm learns from labeled training data to make predictions or decisions.

Supervised learning can be broadly categorized into two main types: **regression** and **classification**.

#### 回归

* regression: 用来预测连续的结果值，比如股票价格、商品销售额、航空公司的路线规划等。评估标准：Evaluation Metrics
  Mean squared error (MSE), mean absolute error (MAE), R-squared

#### 分类

* classification：用来预测离散的、可分类的输出。包括二分类、多类别分类等，甚至一个输入对应多个不同的类别（比如多标签分类）；使用案例有：预测微博文章的情感倾向、良性还是恶性肿瘤、借款的风险等级等。评估标准：Accuracy, precision, recall, F1-score, ROC curve

2.1. 分类又可以进一步区分为三种类型：

* Binary classification：比如信用卡风险评估为发放或者不发放（给予用户的风险登记、信用、财产情况等评估）
* Multiclass classification：分类的类别有三个以上、但是每次结果只有一个类别。常见的应用有比如车牌号识别、Google的手写输入识别等。
* Multilabel classification：分类为多个类别，可以对同一个输入，同时打多个标签。比如同时打财务、技术和商业标签等。

应用：
Sentiment analysis：
Image recognition: Social media platforms like Instagram use classification algorithms to automatically recognize and tag individuals in photographs.
Medical diagnosis：医药诊断

可以通过：accuracy, precision, and recall ,metrics.accuracy_score、precision_score、recall_score来评估模型的效果

#### 有监督学习模型算法

1. 线性回归算法：
    Linear regression、Polynomial regression、Logistic regression

2. 逻辑回归算法：

3. 决策树模型 Decision Tree、

4. Random Forests

5. SVM（Support Vector Machine）：支持向向量机

6. K-Nearest neighbors(KNN):K-邻近算法：
https://www.kaggle.com/code/prashant111/knn-classifier-tutorial#20.-k-fold-Cross-Validation-


### 无监督学习

There’s no target or class attribute. The methods of unsupervised learning are used to find underlying patterns in data and are often used in exploratory data analysis.

In unsupervised learning, the data is not labeled. The methods instead focus on the data’s features. The overall goal of the methods is to find relationships within the data and group data points based on some similarity matrix.

无监督学习可以分为三种类型：

#### 聚类 Clustering

    从没有标签的数据集中，发现蕴含的类别。主要有两类： 1. Partitional clustering：

#### Dimensionlity reduction 降纬算法

#### Association mining 关联挖掘


# 机器学习解决问题的主要过程

1）问题定义
2）数据的收集和预处理
3）选择机器学习模型、
4）训练机器，确定参数
5）超参调试和性能优化


### 构建和使用模型的步骤
The steps to building and using a model are:

- Define: What type of model will it be? A decision tree? Some other type of model? Some other parameters of the model type are specified too.
- Fit: Capture patterns from provided data. This is the heart of modeling.
- Predict: Just what it sounds like
- Evaluate: Determine how accurate the model's predictions are.

基本步骤：

1. 加载数据：进行数据处理和清晰、查看数据是否符合要求、
2. 数据准备：数据预处理，对数据进行清洗、特征工程Feature Scales等
3. 模型训练：
   选择合适的模型（分类、回归 or 决策树等）、把数据拆分为训练数据和评估数据
4. 模型预测：
5. 评估模型预测的效果是否符合需求
   常见的评估模型好坏的指标：Mean Absolute Error (also called MAE)、

```
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error

forest_model = RandomForestRegressor(random_state=1)
forest_model.fit(train_X, train_y)
melb_preds = forest_model.predict(val_X)
print(mean_absolute_error(val_y, melb_preds))
```

注意事项:

1. It’s crucial to randomize the split and ensure the model doesn’t overfit to a specific data subset, which can be achieved by setting shuffle=True.

### 一次完整的模型训练过程

划分数据集：train_test_split

```python
from sklearn.model_selection import train_test_split

# split data into training and validation data, for both features and target
# The split is based on a random number generator. Supplying a numeric value to
# the random_state argument guarantees we get the same split every time we
# run this script.
train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)
# Define model
melbourne_model = DecisionTreeRegressor()
# Fit model
melbourne_model.fit(train_X, train_y)

# get predicted prices on validation data
val_predictions = melbourne_model.predict(val_X)
print(mean_absolute_error(val_y, val_predictions))
```



## 数据预处理

进行缺失数据的补齐、分类数据、标签数据的one-hot encoding等、Normalize and standardize transform to common Scale

> 参考：https://www.kaggle.com/code/alexisbcook/categorical-variables

### 数据 规范化、正则化、标准化
数据规范化 Normalization：一般是把数据限定在需要的范围，比如[0,1],从而消除数据量纲对建模的影响。 
标准化 Standardization：是指将数据正态分布，使平均值为0，标准差为1。
正则化 Regularization：是在损失函数里面加入惩罚项，增加建模的模糊性，从而把捕捉到的局部细微趋势，调整到整体大概趋势。能够有防止模型过拟合的问题。

上面两个都是对数据操作，消除过大数据差异，以及离群数据带来的影响。而正则化则是调整模型，约束权重。


处理日常可能碰到的数据问题：Missing Values、Categorical variables




### Missing Values 解决数据缺失的三种方法
- drop columns：删除对模型没有用的数据

- Imputation:使用均值等来赋值某个缺失的值（更好一点的方法）
   
   该方法很好，但是有个缺陷，有时候可能会系统性的高于、或者低于原本的值。

- 添加一个新列，独立标记是否存在缺失值

更好的方法：补齐缺失值，并且新增一列标记某一行值存在缺失的情况。

```python
# 参考方法2:

from sklearn.impute import SimpleImputer

# Imputation
my_imputer = SimpleImputer()
imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))
imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))

# Imputation removed column names; put them back
imputed_X_train.columns = X_train.columns
imputed_X_valid.columns = X_valid.columns

print("MAE from Approach 2 (Imputation):")
print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))

```

数据准备好了之后：
```
# Define and fit model
model = RandomForestRegressor(n_estimators=100, random_state=0)
model.fit(final_X_train, y_train)

# Get validation predictions and MAE
preds_valid = model.predict(final_X_valid)
print("MAE (Your approach):")
print(mean_absolute_error(y_valid, preds_valid))
```


### Categorical Variables

对于离散的、比如类别类型的数据（如男、女，爱好等），很多机器学习算法模型处理不了，需要进行预处理。

1. drop categorical variables：直接丢弃
2. ordinal encoding：数据编码：对于不同的离散值赋予不同的序数编码(用编号1、2、3等替代离散值)

This assumption makes sense in this example, because there is an indisputable ranking to the categories. Not all categorical variables have a clear ordering in the values, but we refer to those that do as ordinal variables. For tree-based models (like decision trees and random forests), you can expect ordinal encoding to work well with ordinal variables.

3. One-Hot Encoding:（创建新的列来表示是否出现某个离散值）
   One-hot encoding creates new columns indicating the presence (or absence) of each possible value in the original data. 
   例子：比如对于颜色列，分别单独单独创建红列（1，0），黄列（1，0）、绿列（1，0）来表示某个颜色是否存在。

In contrast to ordinal encoding, one-hot encoding does not assume an ordering of the categories. Thus, you can expect this approach to work particularly well if there is no clear ordering in the categorical data (e.g., "Red" is neither more nor less than "Yellow"). We refer to categorical variables without an intrinsic ranking as nominal variables.

当离散类型特别多的时候（通常超过15个）后，就不选择one-hot encoding方法,因为如果类别太多的时候，会增加态度数据，导致数据量爆炸。


参考代码
```python
# 方案1
drop_X_train = X_train.select_dtypes(exclude=['object'])
drop_X_valid = X_valid.select_dtypes(exclude=['object'])

print("MAE from Approach 1 (Drop categorical variables):")
print(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))


# 方案2:
# Make copy to avoid changing original data 
label_X_train = X_train.copy()
label_X_valid = X_valid.copy()

from sklearn.preprocessing import OrdinalEncoder
# Apply ordinal encoder to each column with categorical data
ordinal_encoder = OrdinalEncoder()
label_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])
label_X_valid[object_cols] = ordinal_encoder.transform(X_valid[object_cols])

print("MAE from Approach 2 (Ordinal Encoding):") 
print(score_dataset(label_X_train, label_X_valid, y_train, y_valid))


# 方案3
from sklearn.preprocessing import OneHotEncoder

# Apply one-hot encoder to each column with categorical data
OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)
OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))
OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))

# One-hot encoding removed index; put it back
OH_cols_train.index = X_train.index
OH_cols_valid.index = X_valid.index

# Remove categorical columns (will replace with one-hot encoding)
num_X_train = X_train.drop(object_cols, axis=1)
num_X_valid = X_valid.drop(object_cols, axis=1)

# Add one-hot encoded columns to numerical features
OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)
OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)

# Ensure all columns have string type
OH_X_train.columns = OH_X_train.columns.astype(str)
OH_X_valid.columns = OH_X_valid.columns.astype(str)

print("MAE from Approach 3 (One-Hot Encoding):") 
print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))

```

2. Cleaning和Tidy数据的方法

* melt() 用来把数据转化成类似Key-Value形式，其中Key为ColumnName，Value为该Column的值。
* pivoting()数据，是melt的反操作。


#### Feature Scaling：


## 模型训练

### 模型训练操作的步骤
1. Step1: Define Preprocessing Steps
2. Step2: Define the Model
3. Step3: Create and Evalute the Pipeline

定义处理步骤
```python 
import numpy as np
from sklearn.preprocessing import OrdinalEncoder

# Preprocessing for numerical data
numerical_transformer = SimpleImputer(missing_values=np.nan, strategy='mean') # Your code here

# Preprocessing for categorical data
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('ordinal', OrdinalEncoder())
])  # Your code here

# Bundle preprocessing for numerical and categorical data
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

# Define model
model =  RandomForestRegressor(n_estimators=100, random_state=0)# Your code here

# Check your answer
step_1.a.check()
```
### pipeline 组织模型训练
pipeline的优势：
1. Cleaner Code
2. Fewer Bugs
3. Easier to Productionize
4. More Options for Model Validation

### Cross Validation

k-fold Cross validation
当数据比较少的时候，为了充分利用好我们的数据，同时又不影响最终的实验结果。选择将训练数据分层k份，然后产生k个模型，然后每个模型依次在其中k-1份数据上训练，在剩下的k份数据上验证。

适用范围：
* For small datasets, where extra computational burden isn't a big deal, you should run cross-validation.
* For larger datasets, a single validation set is sufficient. Your code will run faster, and you may have enough data that there's little need to re-use some of it for holdout.

怎么区分数据集大小：
But if your model takes a couple minutes or less to run, it's probably worth switching to cross-validation.


## XGBoost
使用gradient boosting（梯度提升）方法来优化模型、
**ensemble methods** combine the predictions of several models (e.g., several trees, in the case of random forests).

   

## leakage 数据泄露
在训练的时候包含了目标数据，导致训练出的模型在测试验证的时候效果很好，但是实际应用预测的时候，准确率很差。

包含两种：target leakage and train-test contamination.




# 怎么评估模型的好坏

评估模型预测的准确性。比较预测值和实际值之间的差距来评估模型的好坏。比如用MAE来评估。

1. MAE：The mean absolute error (MAE) can be computed from this data to represent the average absolute variance between the projected and actual values. A lower MAE value signifies more precise predictions. Although there are other methods for validating models, they won’t be examined in this context.

```python
from sklearn.metrics import mean_absolute_error

predicted_home_prices = melbourne_model.predict(X)
mean_absolute_error(y, predicted_home_prices)
```

通过多次调整模型的参数，然后来比较预测结果的mae值，最终选中一个最好的参数，然后用这个参数部署到线上。

### 过拟合和欠拟合

#### 过拟合 Overfitting

This is a phenomenon called **overfitting**, where a model matches the training data almost perfectly, but does poorly in validation and other new data.

Overfitting: capturing spurious patterns that won't recur in the future, leading to less accurate predictions, or

在训练数据中匹配的很好，但是模型不太适合验证数据，导致效果不好。

#### 欠拟合： Underfitting：

When a model fails to capture important distinctions and patterns in the data, so it performs poorly even in training data, that is called underfitting.

Underfitting: failing to capture relevant patterns, again leading to less accurate predictions.

在训练数据中也表现的不太好。

## 怎么评估分类算法的优劣

### Loss Function 损失函数
损失函数：把预测值和真实值进行比较，两者的差值作为损失函数。

### Cost Function 
the cost function is summation of loss function

### Confusion matrix

混淆矩阵：用于评估分类算法的性能

A confusion matrix is a tool for summarizing the performance of a classification algorithm. A confusion matrix will give us a clear picture of classification model performance and the types of errors produced by the model. It gives us a summary of correct and incorrect predictions broken down by each category. The summary is represented in a tabular form.

四种分类结果：

* True Positives（TP）:真的分类为真的

* True Negatives（TN）：负的分类为负的

* False Positives（FP）： False Positives occur when we predict an observation belongs to a certain class but the observation actually does not belong to that class. This type of error is called **Type I error.**

* False Negatives (FN)：False Negatives occur when we predict an observation does not belong to a certain class but the observation actually belongs to that class. This is a very serious error and it is called **Type II error.**

### Classification metrices

使用Classification Report，统计模型的：precision、recall、f1和support scores

#### Precision[¶](https://www.kaggle.com/code/prashant111/knn-classifier-tutorial#Precision)

**Precision** can be defined as the percentage of correctly predicted positive outcomes out of all the predicted positive outcomes. It can be given as the ratio of true positives (TP) to the sum of true and false positives (TP + FP).

```py
precision = TP / float(TP + FP)
```

#### Recall 召回率

Recall can be defined as the percentage of correctly predicted positive outcomes out of all the actual positive outcomes. It can be given as the ratio of true positives (TP) to the sum of true positives and false negatives (TP + FN). **Recall** is also called **Sensitivity**.

```py
recall = TP / float(TP + FN)
```

#### f1-score[¶](https://www.kaggle.com/code/prashant111/knn-classifier-tutorial#f1-score)

**f1-score** is the weighted harmonic mean of precision and recall. The best possible **f1-score** would be 1.0 and the worst would be 0.0. **f1-score** is the harmonic mean of precision and recall. So, **f1-score** is always lower than accuracy measures as they embed precision and recall into their computation. The weighted average of `f1-score` should be used to compare classifier models, not global accuracy.

### ROC Curve

ROC Curve stands for **Receiver Operating Characteristic Curve**.

### ROC AUC[¶](https://www.kaggle.com/code/prashant111/knn-classifier-tutorial#ROC--AUC)

[Python Machine Learning - AUC - ROC Curve](https://www.w3schools.com/python/python_ml_auc_roc.asp)

**ROC AUC** stands for **Receiver Operating Characteristic - Area Under Curve**.

### R2分数？？





# 常见模型算法

问题：
1. 各种模型的优缺点？
2. 

## 线性回归
## 聚类方法

## 决策树：

如果树的深度过深，容易导致过拟合。

## 随机森林

相关参数：样本数N、特征数M
采用有放回抽样，随机选择m个特征（m远小于M）。节点生成的时候不总是考虑全部特征，和有放回抽样，这两个关键点增加了树生成过程中的随机性，从而降低了过拟合。

随机模型的优势
1. 模型准确性比较高，因为底层的规则系统，准确率比较高。
2. 不太容易收到异常值和脏数据的干扰。 异常值的干扰被平摊了。因为采用随机采样的方式来避规了。
3. 不需要去做标准化，因为是分叉，数值的权重不会影响树的分叉。
4. 可以处理一些高纬度的特征，数据特征的相关不会受到影响。不像线性回归，数据特征中不能强相关、相关性需要处理。
5. 可解析性，特征重要性可解释、决策树分叉逻辑可以展示出来。
6. 既可以处理回归、也可以处理分类问题。回归问题返回一个平均数值。

```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error

forest_model = RandomForestRegressor(random_state=1)
forest_model.fit(train_X, train_y)
melb_preds = forest_model.predict(val_X)
print(mean_absolute_error(val_y, melb_preds))
```


## K-Means 算法（聚类算法）

K-Means 聚类方法，用来将数据分为k类书籍。参考ex文件夹总的使用例子。

参考：[我的K-Means用例](https://github.com/jasonye/learning_note/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ex/Proj_1_K-Means.ipynb)

K-means is an unsupervised learning method for clustering data points. The algorithm iteratively divides data points into K clusters by minimizing the variance in each cluster.

### 手肘法--找k值

K-means方法最重要的是找到需要聚集数-k，最常用的方法是使用手肘法，通过赋值1～n为k值，然后计算聚类后的距离值。

具体实践：参考这里：[Python Machine Learning - K-means](https://www.w3schools.com/python/python_ml_k-means.asp)，按下面的步骤计算：

1. 通过绘制图形观察可能的聚类数量、
2. 通过手肘法、判断图形挂点找到对应的k值数
3. 然后通过k值数拟合训练数据、
4. 然后进行预测predict

Demo样类：

```python
# 生成测试数据

from sklearn.datasets import make_blobs

X, y = make_blobs(n_samples=100, n_clusters=3, center_box=(-10, 10), std_dev=1.0)
```

## KNN---K邻近算法（分类和回归算法）

参考文档：[kNN Classifier Tutorial | Kaggle](https://www.kaggle.com/code/prashant111/knn-classifier-tutorial#12.-Feature-Scaling-)

KNN is a simple, **supervised machine learning** (ML) algorithm that can be used for **classification** or **regression tasks** - and is also frequently used in missing value imputation. It is based on the idea that the observations closest to a given data point are the most "similar" observations in a data set, and we can therefore classify unforeseen points based on the values of the closest existing points. By choosing *K*, the user can select the number of nearby observations to use in the algorithm.

- In kNN classification, the output is a class membership. The given data point is classified based on the majority of type of its neighbours. The data point is assigned to the most frequent class among its k nearest neighbours. Usually k is a small positive integer. If k=1, then the data point is simply assigned to the class of that single nearest neighbour.

- In kNN regression, the output is simply some property value for the object. This value is the average of the values of k nearest neighbours.

n kNN algorithm, k is the number of nearest neighbours. Generally, k is an odd number because it helps to decide the majority of the class. When k=1, then the algorithm is known as the nearest neighbour algorithm.

1. 怎么选择参数k- 最重要的问题
   
   A small value of k means that noise will have higher influence on the result. So, probability of overfitting is very high. 
   
   A large value of k makes it computationally expensive in terms of time to build the kNN model. Also, a large value of k will have a smoother decision boundary which means lower variance but higher bias.
   
   We can apply the elbow method to select the value of k. To optimize the results, we can use Cross Validation technique. Using the cross-validation technique, we can test the kNN algorithm with different values of k. The model which gives good accuracy can be considered to be an optimal choice. It depends on individual cases and at times best process is to run through each possible value of k and test our result.


# 实践项目
* 项目1: 学习使用K-Means算法进行数据聚类，具体见ex目录实验
* 项目2: kNN k邻近算法实验进行分类和回归，具体见ex目录实验
* 项目3: 进行数字1-9的识别和分类（区分数字0～1、2～3）
        可以进行是这进行车牌号码的识别？
* 项目4: 进行猫和狗的分类、 卷积神经网络
* 项目5: 进行10种狗狗类别识别、分类、 卷积神经网络
* 项目5: 进行文本情感分析、分类、语音识别？ （循环神经网络）
* 项目6: 进行人脸识别等 



## 参见问题和解决
1. ModuleNotFoundError: No module named 'sklearn'
   
```python
   
pip3 install scikit-learn 
```

# 待了解研究内容


1. 基础：介绍Python、Pandas、Numpy等工具的使用，数据清洗方法等
https://www.kaggle.com/code/kanncaa1/data-sciencetutorial-for-beginners

2. 待学习seaborn库的使用：https://www.kaggle.com/code/kanncaa1/seaborn-tutorial-for-beginners?scriptVersionId=27768785

6. Data Visualization
Seaborn: https://www.kaggle.com/kanncaa1/seaborn-for-beginners
Bokeh 1: https://www.kaggle.com/kanncaa1/interactive-bokeh-tutorial-part-1
Rare Visualization: https://www.kaggle.com/kanncaa1/rare-visualization-tools
Plotly: https://www.kaggle.com/kanncaa1/plotly-tutorial-for-beginners

R 语言是什么？

6. Machine Learning: https://www.kaggle.com/code/kanncaa1/machine-learning-tutorial-for-beginners

7. Deep Learning
ANN 预测癌症、

https://www.kaggle.com/code/thebrownviking20/intro-to-keras-with-breast-cancer-data-ann


https://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners

CNN for dummies
https://medium.com/@prathammodi001/convolutional-neural-networks-for-dummies-a-step-by-step-cnn-tutorial-e68f464d608f

CNN课程：
https://www.kaggle.com/code/kanncaa1/convolutional-neural-network-cnn-tutorial

RNN for beginner
https://www.kaggle.com/code/rafetcan/recurrent-neural-n-rnn-tutorial-for-beginners

RNN with Pytorch：
https://www.kaggle.com/code/kanncaa1/recurrent-neural-network-with-pytorch
RNN 视频课程
https://www.youtube.com/watch?v=UNmqTiOnRfg&t=3s


8. Time Series Prediction
https://www.kaggle.com/kanncaa1/time-series-prediction-tutorial-with-eda

时间序列
https://www.kaggle.com/code/thebrownviking20/everything-you-can-do-with-a-time-series

9. Deep Learning with Pytorch
Artificial Neural Network: https://www.kaggle.com/kanncaa1/pytorch-tutorial-for-deep-learning-lovers
Convolutional Neural Network: https://www.kaggle.com/kanncaa1/pytorch-tutorial-for-deep-learning-lovers
Recurrent Neural Network: https://www.kaggle.com/kanncaa1/recurrent-neural-network-with-pytorch