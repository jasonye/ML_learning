# # 机器课程

[toc]

## 学习机器学习基础

熟悉机器模型训练过程、熟悉使用Pandas、Numpy、Sci-learnging库等、参加机器学习比赛等

1. [Learn Intro to Machine Learning Tutorials | Kaggle](https://www.kaggle.com/learn/intro-to-machine-learning)

## 中级机器学习课程

Pandas使用

# 大模型课程

# 机器学习和LLM学习笔记tu推荐文档

学习文档：https://github.com/WSake/LLM?tab=readme-ov-file

对应的英文文档：https://github.com/mlabonne/llm-course

吴恩达课程：https://blog.csdn.net/weixin_46460463/article/details/140536677

交大课程：https://gair-nlp.github.io/cs2916/docs/intro

机器学习社区：https://www.kaggle.com/

学习文档：
https://www.digitaloceans.cn/technology/355/

学习文档：
https://github.com/rasbt/LLMs-from-scratch

https://github.com/karpathy/LLM101n

### 重点文档：

1. 生成式AI入门：吴恩达的课程：
    https://www.deeplearning.ai/courses/generative-ai-for-everyone/

2. https://www.bilibili.com/video/BV1fw4m1Y76e/?spm_id_from=333.1387.upload.video_card.click&vd_source=b0146a2f6a5c8d8a41b9d594948e2858

3. 吴恩达讲大模型：

4. https://space.bilibili.com/1786444896

5. Google的基础可能：https://www.cloudskillsboost.google/paths/118

大模型微调课程：
https://www.bilibili.com/video/BV1kJWUeoEuG/?vd_source=b0146a2f6a5c8d8a41b9d594948e2858

PyTorch的：
https://www.nvidia.cn/glossary/pytorch/

# 机器学习

参考课程：
7. 五天GenAI课程： https://www.kaggle.com/learn-guide/5-day-genai
8. Tensorflow课程: https://www.kaggle.com/learn-guide/tensorflow

1. https://www.kaggle.com/learn/intro-to-machine-learning (完成学习，实践待继续)

2. https://www.kaggle.com/learn/intermediate-machine-learning

3. 数据可视化：https://www.kaggle.com/learn/data-visualization

4. Feature-engineering https://www.kaggle.com/learn/feature-engineering

5. 深度学习：https://www.kaggle.com/learn/intro-to-deep-learning

6. https://www.kaggle.com/learn/intro-to-game-ai-and-reinforcement-learning

ML with NumPy、pandas、scikit-learn and More

https://www.educative.io/courses/machine-learning-numpy-pandas-scikit-learn

NLP课程：
https://www.kaggle.com/learn-guide/natural-language-processing
https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners


交通大学课程：动手学习大模型：
https://github.com/Lordog/dive-into-llms/blob/main/documents/chapter1/README.md

# 阿里云人工智能学习大纲和路线

https://developer.aliyun.com/learning/roadmap/ai

1. 机器学习入门「常见算法和神经网络大纲」
2. 动手实践：深度学习框架TensorFlow框架入门和常用库{NumPy入门、Panda、Matplotlib等}
3. 机器学习实践：熟练掌握学习算法、神经网络的实践应用、：推荐系统、新闻分类、随机森林的气温预测等、
4. 自然语言处理实践：人机对话、文本相似度分析、机器翻译、AI写唐诗、AI输入法等
5. 图像识别实践：神经网络：人脸识别、猫狗识别、行为识别、验证码识别、图像修复等
6. 大预言模型：大模型使用等、https://edu.aliyun.com/course/3126500/


# 动手实践：
1. 使用langchain工具、
2. 部署使用第一个transformer模型、
3. pytorch工具的使用等、
2. 使用RAG、
3. 实践MCP
* 实践使用Ollama，部署其他的大语言模型，比如ds r1等，参考文档：https://github.com/ollama/ollama

# 待了解技术方案
模型和框架： 
* 模型：Transformer、GPT、BERT模型原理、
* 框架：PyTorch、TensorFlow框架
* 工具链：LangChain、LamaIndex、AutoGen等Agent开发工具、
工程实践：
* RAG、Prompt工程


## 目前热门的LLM模型
* ChatGPT(OpenAI)
* Gemini(Google)
* DeepSeek-R1
* Grok?
* LLama?
* Claude 是由人工智能公司 Anthropic 开发的大型语言模型（LLM）助手，类似于 ChatGPT（OpenAI）、Gemini（Google）或我 DeepSeek-R1（深度求索）。你可以把它看作是一个智能聊天机器人，具备强大的文本理解与生成能力，能帮你完成各种任务，

# 待学习内容：
https://github.com/naklecha/llama3-from-scratch


# 当前AI工具和平台

* 豆包
* DeepSeek
* 智谱大模型： 清华大学团队创建的大模型公司。https://open.bigmodel.cn/
* Coze
* Kimi：月之暗面23年10月9日推出的一款智能助手，主要应用场景为专业学术论文的翻译和理解、辅助分析法律问题、快速理解API开发文档等，是全球首个支持输入20万汉字的智能助手产品。
* Cursor: 
* Claude: 什么是Claude模型？


# 概念

* 机器学习：Machine learning is a subfield of artificial intelligence that focuses on creating models and algorithms that allow computers to learn from and make predictions or decisions on the basis of data.

* 深度学习：Deep learning is an area of machine learning that applies artificial neural networks inspired by the functions of the human brain.

* 强化学习和机器学习的区别

强化学习和机器学习是两个相关但不同的领域。以下是它们的主要区别：

1. 定义
   机器学习 (Machine Learning)：是一种使用算法从数据中学习的技术，旨在使计算机能够根据输入数据预测输出。机器学习通常分为监督学习、无监督学习和半监督学习等类型。
   强化学习 (Reinforcement Learning)：是一种特殊类型的机器学习，关注于如何基于环境反馈进行决策。强化学习中的智能体通过与环境互动来学习，目标是最大化累积奖励。
2. 学习方式
   机器学习：通常依赖于静态数据集进行训练，通过输入和对应的标签进行学习。
   强化学习：智能体在环境中采取行动，接收奖励或惩罚，并基于这些反馈不断调整策略。
3. 应用场景
   机器学习：广泛应用于图像识别、自然语言处理、推荐系统等。
   强化学习：常用于游戏（如围棋、电子游戏）、机器人控制、自动驾驶等需要实时决策的场景。
4. 反馈机制
   机器学习：使用明确的标签进行反馈。
   强化学习：通过奖励信号进行反馈，可能没有明确的标签，且反馈通常是延迟的。
5. 目标
   机器学习：通常目标是最小化预测误差。
   强化学习：目标是最大化长期累积奖励。
   总结来说，强化学习是机器学习的一个子领域，关注于通过试错和反馈学习策略，而机器学习则是一个更广泛的概念，涉及多种学习方式。


* 神经网络 neural network

* **stochastic gradient descent**：随机梯度下降；随机梯度下降法？？

* **dropout**, **batch normalization**

* NLP：Natural Language Processing(nlp) 自然语言处理

* training / fitting

* training data

* predict 预测

* 过拟合 Overfitting：This is a phenomenon called overfitting, where a model matches the training data almost perfectly, but does poorly in validation and other new data. 

* 欠拟合： Underfitting：When a model fails to capture important distinctions and patterns in the data, so it performs poorly even in training data, that is called underfitting.

Overfitting: capturing spurious patterns that won't recur in the future, leading to less accurate predictions, or
Underfitting: failing to capture relevant patterns, again leading to less accurate predictions.

* One-hot encoding: 将词汇表中出现的token进行0/1化表示，出现了则为1，未出现则为0。 
* Bag-of-words模型：

## 等待研究词语：

* n-gram table：
* RNNS：循环神经网络？recurrent neural networks、
* LSTM：Long short-term memory：RNN神经网络的一种，时序相关的神经网络
* GRU：gated recurrent unit：门控循环单元架构：？？

### 

## 数据清洗的方法

参考：https://www.kaggle.com/code/alexisbcook/categorical-variables

1. drop columns
2. imputation：使用均值等来赋值某个缺失的值、
3. 添加一个新的列，独立标记缺失的值

### Categorical Variables

对于离散的、比如类别类型的数据（如男、女，爱好等），很多机器学习算法模型处理不了，需要进行预处理。

1. drop categorical variables
2. ordinal encoding：对于不同的离散值赋予不同的序数编码

This assumption makes sense in this example, because there is an indisputable ranking to the categories. Not all categorical variables have a clear ordering in the values, but we refer to those that do as ordinal variables. For tree-based models (like decision trees and random forests), you can expect ordinal encoding to work well with ordinal variables.

3. One-Hot Encoding 
   One-hot encoding creates new columns indicating the presence (or absence) of each possible value in the original data. 
   例子：比如对于颜色列，分别单独单独创建红列（1，0），黄列（1，0）、绿列（1，0）来表示某个颜色是否存在。

In contrast to ordinal encoding, one-hot encoding does not assume an ordering of the categories. Thus, you can expect this approach to work particularly well if there is no clear ordering in the categorical data (e.g., "Red" is neither more nor less than "Yellow"). We refer to categorical variables without an intrinsic ranking as nominal variables.

当离散类型特别多的时候（通常超过15个）后，就不选择one-hot encoding方法。

# LLM

## Agent



# 名词和概念

- 机器学习：Machine learning is a subfield of artificial intelligence that focuses on creating models and algorithms that allow computers to learn from and make predictions or decisions on the basis of data.

- 深度学习：Deep learning is an area of machine learning that applies artificial neural networks inspired by the functions of the human brain.

- 神经网络 neural network

- **stochastic gradient descent**：随机梯度下降；随机梯度下降法？？

- **dropout**, **batch normalization**

- NLP：Natural Language Processing(nlp) 自然语言处理

- training / fitting

- training data

- predict 预测

- 过拟合 Overfitting：This is a phenomenon called overfitting, where a model matches the training data almost perfectly, but does poorly in validation and other new data.

- 欠拟合： Underfitting：When a model fails to capture important distinctions and patterns in the data, so it performs poorly even in training data, that is called underfitting.

        Overfitting: capturing spurious patterns that won't recur in the future, leading to less accurate predictions, or
        Underfitting: failing to capture relevant patterns, again leading to less accurate predictions.

- One-hot encoding: 将词汇表中出现的token进行0/1化表示，出现了则为1，未出现则为0。

- Bag-of-words模型：

- n-gram table：

- RNNS：循环神经网络？recurrent neural networks、

- LSTM：Long short-term memory 架构：？？

- GRU：gated recurrent unit：门控循环单元架构：？？






## 向量数据库

参考介绍文档： https://zhuanlan.zhihu.com/p/643536744
参考文档： https://zhuanlan.zhihu.com/p/40487710

我们在用图片搜索图片，或者语音搜索语音的时候，在数据库中存储和对比的并不是图片和语音片段，而是通过DL等算法提取出来的“特征”，一般是256/512个float数组，可以用数学中的向量来表示。

向量数据库就是用来存储，检索，分析向量的数据库。只所以称之为数据库，是因为它有下面几个特征：

a) 提供标准的sql访问接口，降低用户的使用门槛

b）提供高效的数据组织，检索和分析的能力。一般用户在存储和检索向量的同时，还需要管理结构化的数据，即支持传统数据库对结构化数据的管理能力。



# 梯度的理解

- 什么是梯度？
  
  - 用户可能会混淆导数和梯度，得区分清楚：导数是标量函数对单个变量的变化率，梯度则是多元函数对所有变量的偏导数组成的向量。比如山坡的例子就很直观 - 坡度最陡的方向就是梯度方向。

- 什么是梯度提升树？

在机器学习中，**梯度**是一个核心的数学概念，尤其在优化算法（如梯度下降）中起着至关重要的作用。理解梯度是理解模型如何“学习”的关键。

### 1. **基本定义**

- **梯度（Gradient）** 是一个**向量**（一组有序的数）。

- 它表示一个**多元函数**（输入是多个变量的函数）在其定义域内某一点处**变化率最大（最陡峭）的方向**以及该方向上的**变化率大小**。

- 具体来说，对于一个函数 `f(x₁, x₂, ..., xn)`，其在点 `(a₁, a₂, ..., an)` 处的梯度是一个向量，记作 `∇f(a)` 或 `grad f(a)`。这个向量的每个分量是该函数在该点**对各自变量的偏导数**：  
  `∇f(a) = [∂f/∂x₁(a), ∂f/∂x₂(a), ..., ∂f/∂xn(a)]ᵀ`

### 2. **直观理解**

- 想象你站在一座山上。函数 `f` 的值代表你所在位置的海拔高度。

- **梯度** `∇f` 在你当前位置所指向的方向，就是你**面朝哪个方向走一步（步长非常小），海拔会上升得最快（最陡峭的上坡）**。

- 梯度的**长度（模）** 则代表了那个方向上坡的**陡峭程度**。梯度越长，坡越陡；梯度越短（接近零），地势越平坦。

- **关键点：** 梯度指向函数值**增长最快**的方向。在机器学习中，我们通常需要**最小化**一个损失函数（Loss Function），因此我们关心的是梯度的**反方向（负梯度 `-∇f`）**，因为这个方向指向函数值**下降最快**的方向（最陡峭的下坡）。

### 3. **在机器学习中的核心作用（优化）**

机器学习的核心目标之一是找到一组模型参数（如线性回归中的权重 `w` 和偏置 `b`，神经网络中的权重和偏置），使得模型在训练数据上的**损失函数 `L(θ)`** 的值达到**最小**（或非常接近最小）。这里的 `θ` 代表所有需要优化的参数集合。

- **梯度下降算法：** 这是最常用的优化算法。其核心思想就是利用梯度：
  
  1. **计算梯度：** 在当前位置 `θ_t`，计算损失函数 `L` 关于所有参数 `θ` 的梯度 `∇L(θ_t)`。这个梯度告诉我们损失函数在当前参数设置下，**在参数空间中，损失上升最快的方向**。
  
  2. **向反方向更新：** 为了最小化损失，我们需要沿着**负梯度方向（`-∇L(θ_t)`）** 更新参数：`θ_{t+1} = θ_t - η * ∇L(θ_t)`
  
  3. **重复：** 使用新的参数 `θ_{t+1}` 重复步骤 1 和 2，直到损失函数收敛（不再显著下降）或达到预设的迭代次数。

- 其中 `η` 是**学习率**，一个超参数，控制每次更新的步长有多大。

### 4. **为什么梯度如此重要？**

- **导航：** 在复杂的高维参数空间中（可能有数百万甚至数十亿个参数），梯度提供了在当前位置找到“下坡路”的最直接、最有效的方向信息。没有梯度，优化就像在黑暗中摸索。

- **效率：** 梯度下降（及其变种）利用梯度信息，能以相对高效的方式找到损失函数的（局部）最小值点。

- **可计算性：** 对于由可微运算（如加、乘、常见激活函数等）构成的模型（如神经网络），可以利用**反向传播算法**极其高效地计算出损失函数关于所有参数的梯度 `∇L(θ)`。这是深度学习得以实现的关键技术之一。

### 5. **梯度相关的关键概念**

- **梯度消失/爆炸：** 在训练深层神经网络时常见的问题。梯度在反向传播过程中逐层计算。如果梯度值非常小（接近0），随着层数加深会变得越来越小（消失），导致深层参数几乎不更新。如果梯度值非常大，随着层数加深会变得巨大（爆炸），导致参数更新过大，模型不稳定。这是设计网络结构和选择激活函数（如 ReLU 缓解了消失问题）时需要考虑的重要因素。

- **梯度下降变种：** 为了解决标准梯度下降（批量梯度下降）在大数据集上的效率问题，发展出了随机梯度下降（SGD）、小批量梯度下降（Mini-batch SGD）以及更先进的优化器如 Momentum、RMSProp、Adam 等。它们都基于梯度信息，但采用了不同的更新策略。

- **驻点：** 梯度为零（`∇f = 0`）的点。这可能是函数的**局部最小值点、局部最大值点或鞍点**。梯度下降法在达到驻点时就会停止更新。我们希望找到全局最小值点或好的局部最小值点。

### 总结

简而言之，在机器学习中：

- **梯度**是一个向量，指示了**损失函数在参数空间中当前点处上升最快的方向和速率**。

- 为了**最小化损失函数**，我们沿着梯度的**反方向（负梯度方向）** 更新模型参数。

- **梯度下降**及其变种是利用梯度信息进行优化的基础算法。

- **计算梯度**（通常通过反向传播实现）是训练神经网络的核心计算步骤。

- 理解梯度是理解模型如何通过数据“学习”和改进其预测能力的基石。

可以把梯度看作是优化过程中的“指南针”，它不断地告诉模型：“为了更快地降低错误，你的参数应该朝哪个方向调整以及调整多少。”






# 大模型微调：
LoRA


# llama和llama.cpp 的关系

llama 和 llama.cpp 是两个密切相关但扮演不同角色的项目，核心关系是：llama.cpp 是为 Meta 的 LLaMA 模型家族提供高效推理的开源 C++ 实现和工具库。

以下是它们的详细关系和区别：

Meta 的 LLaMA：

是什么： Meta (Facebook) 开发并开源的一系列大型语言模型。包括不同参数规模的版本 (如 7B, 13B, 33B, 65B, 以及后续的 LLaMA 2 系列)。

核心： 模型架构本身、训练好的权重文件。

开源形式： Meta 发布了模型架构（论文）和模型权重（需申请获取）。最初的官方参考实现是 基于 PyTorch 的。

特点： 强大的基础模型，但原生 PyTorch 实现运行需要较大的 GPU 显存资源。

llama.cpp：

是什么： 一个由社区主导的开源项目。

核心目标：

用 纯 C/C++ 重新实现 LLaMA 模型的推理。

对模型进行 量化，显著减小模型大小和降低运行资源需求。

高度优化，支持在 CPU 上高效运行，也可利用 GPU (通过 Metal, CUDA, Vulkan 等后端)。

实现广泛的 跨平台支持 (Windows, Linux, macOS, iOS, Android, WebAssembly 等)。

提供简单的 API 和命令行工具，方便本地运行模型。

与 LLaMA 的关系：

模型来源： llama.cpp 本身不包含模型权重。它加载运行的是从 Meta 官方 LLaMA 或 LLaMA 2 模型 转换而来 的量化模型文件 (通常是 .gguf 格式，早期是 .ggml 格式)。

实现基础： 它重新实现了 LLaMA 模型架构所需的计算操作 (如注意力机制、前馈网络等)，使其能在 C/C++ 环境中高效执行。

关键贡献 - 量化： llama.cpp 的核心价值在于其强大的量化工具链。它能将原始的 FP16 或 BF16 模型权重转换为更低精度的格式 (如 4-bit, 5-bit, 8-bit 等)。这极大地降低了模型的内存占用和计算需求，使得在普通电脑 (甚至手机) 的 CPU 或集成显卡上运行数十亿参数的大模型成为可能。

推理引擎： 它是一个高效的推理引擎，专注于执行已量化、转换好的 LLaMA 模型。

简单总结关系：

LLaMA (Meta): 提供模型的“大脑”（架构和原始权重）。

llama.cpp: 提供一套工具和方法，将这个“大脑” 瘦身（量化） 并 高效地安装运行（C++ 推理引擎） 在各种设备上，尤其是资源受限的设备。

类比：

LLaMA 模型： 就像一台设计精良但耗电量巨大的高性能发动机（原始模型）。

llama.cpp： 就像一位工程师，他做两件事：

改装发动机： 他研究这台发动机，然后制造出一个功能基本相同但体积更小、油耗（资源消耗）低得多的版本（量化模型）。

制造适配的车架： 他设计制造了一个轻量化、高度优化的车架和传动系统（C++ 推理引擎），专门用来安装和高效运行他改装后的发动机，并且这个车架能在各种路况（平台）上行驶。

因此，可以说：

llama.cpp 是 运行 Meta 的 LLaMA 模型 的一种极其流行且重要的方式。

没有 LLaMA 模型，llama.cpp 就没有模型可以运行。

没有 llama.cpp (或其他类似优化)，在普通设备上高效运行 LLaMA 模型会困难得多。

llama.cpp 极大地推动了 LLaMA 模型在本地设备上的普及和应用。

后续发展：

llama.cpp 的成功使其不仅仅局限于运行 LLaMA 模型。现在它也支持加载和运行许多其他兼容架构的开源大模型，如 Mistral、Mixtral、Falcon、MPT、StableLM、Gemma、Phi 等。它已成为一个通用的、高效的本地大模型推理引擎。

它定义了自己的量化格式标准 (GGML -> GGUF)，并被社区广泛采用。

它提供了各种语言的绑定，如 Python (llama-cpp-python)，方便在其他语言中使用。

总而言之，llama.cpp 是围绕 Meta LLaMA 模型生态发展起来的一个至关重要的优化推理项目，它让强大的大模型能够在普通用户的设备上本地高效运行。


## llama.cpp和Ollama的关系
Ollama 和 llama.cpp 是两个紧密相关但定位不同的开源项目，它们共同服务于同一个目标：在本地设备上高效运行大型语言模型（LLMs）。它们的关系可以概括为：

llama.cpp 是底层高效推理引擎，而 Ollama 是在其基础上构建的、更用户友好的模型管理和运行框架。

以下是详细解释和对比：

llama.cpp (核心引擎):

是什么： 一个用 C/C++ 编写的高性能开源库，专注于本地 LLM 推理。

核心功能：

实现了多种 LLM 架构（如 LLaMA, LLaMA 2, Mistral, Mixtral, Gemma, Phi 等）的高效计算。

提供强大的模型量化工具，将模型压缩到更小尺寸、降低资源需求（如 4-bit, 5-bit, 8-bit 量化）。

支持多种硬件后端：CPU（高度优化）、GPU（通过 Metal, CUDA, Vulkan, OpenCL, SYCL）。

跨平台（Windows, Linux, macOS, iOS, Android, WebAssembly）。

定义和使用了 GGUF 模型文件格式（替代早期的 GGML）。

接口： 主要提供 C API 和命令行工具。虽然也有 Python 绑定（llama-cpp-python），但核心是底层库。

用户交互： 相对底层，用户需要手动下载模型（GGUF格式）、管理模型路径、使用命令行参数启动推理。

Ollama (用户友好框架):

是什么： 一个开源的、专注于简化本地 LLM 运行和管理的框架/工具。

核心功能：

模型管理： 内置模型仓库功能。用户只需一个简单的命令（如 ollama pull llama3 或 ollama pull mistral），Ollama 就会自动下载预量化好的 GGUF 模型文件（通常来自 Hugging Face 等社区源）。用户无需手动寻找、下载和管理模型文件。

简化运行： 提供极其简单的命令行（如 ollama run llama3）启动模型对话。隐藏了复杂的命令行参数。

REST API： 提供标准的 RESTful API 接口（默认端口 11434），方便其他应用程序（如聊天UI前端、脚本、开发框架）通过 HTTP 调用本地运行的模型。这大大简化了集成。

运行环境管理： 负责启动和停止模型服务进程。

本地库： 提供官方的 Go 库和 Python 库（ollama package），方便在代码中直接调用模型。

底层依赖： Ollama 的核心推理引擎就是 llama.cpp！ Ollama 内部集成了 llama.cpp 作为其执行模型推理的计算后端。当你通过 Ollama 运行一个模型时，它本质上是在后台调用 llama.cpp 库来处理模型加载和文本生成。

用户交互： 高度抽象化和用户友好，大大降低了使用门槛。

它们的关系总结：

依赖关系： Ollama 重度依赖 llama.cpp。llama.cpp 是 Ollama 能够高效运行各种量化模型的技术基础。Ollama 负责“用户交互”和“模型管理”，而实际的“计算工作”交给 llama.cpp 完成。

分工协作：

llama.cpp: 提供跨平台、高性能、低资源消耗的模型推理能力（核心计算引擎）。

Ollama: 提供模型发现、下载、版本管理、服务化部署、简化命令行交互和 API 接入（用户体验层和模型管理层）。

目标用户：

llama.cpp: 更适合开发者、技术爱好者、需要深度定制或研究底层实现的人，或者需要将推理引擎嵌入到其他 C++/C 项目中的场景。

Ollama: 目标用户更广泛，包括开发者（尤其是需要快速集成 API 的）、研究人员、技术爱好者以及任何希望以最简单方式在本地体验和运行各种开源 LLM 的用户。它极大简化了入门流程。

模型格式： 两者都使用 GGUF 格式作为标准的量化模型文件格式。Ollama 下载的模型就是 GGUF 文件，存放在其本地缓存目录中。

类比：

llama.cpp 是汽车的发动机和传动系统： 提供核心动力和驱动能力，性能卓越且高效，但用户需要了解如何操作离合、换挡等。

Ollama 是整辆汽车（带自动变速箱）： 它把强大的发动机（llama.cpp）封装起来，加上方向盘、油门、刹车、仪表盘、GPS（模型仓库、CLI、API）。用户只需踩油门（ollama run）就能轻松驾驶，无需关心引擎内部如何工作，还能享受自动导航（自动下载模型）和舒适性功能（API）。

简单来说：

你想深入研究底层、追求极致性能或做嵌入式开发？用 llama.cpp。

你想最快速、最简单地在本地电脑上运行 LLaMA 3、Mistral、Gemma 等模型，或者需要一个本地模型 API 服务来集成到你的应用中？Ollama 是你的最佳选择，它内部完美利用了 llama.cpp 的强大能力。

两者都是开源生态中极其重要的项目，共同推动了本地大模型的普及和应用。Ollama 的出现极大地降低了 llama.cpp 的使用门槛。






# 工程实践

1. LangChain的使用
2. RAG 和 MCP



## Ollama

通过Ollama本地跑LLM

> 参考文档： https://ollama.com/
> 参考文档：https://www.llamafactory.cn/ollama-docs/README_ZH.html#%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8


 **Ollama是一款可以方便获取并在本地运行大模型的工具**，支持多种先进的语言模型，包括但不限于Qwen、Llama、DeepSeek-R1, Phi-4、Mistral、Gemma2等。可以让用户能够在服务器中运行使用这些模型。

Ollama是一个**开源的本地大语言模型运行框架**，设计用于简化在本地运行大型语言模型的过程，降低使用门槛。
它通过简单的安装指令，使得用户可以通过一条命令在本地运行开源大型语言模型，例如Llama2。Ollama保留了类似Docker的操作习惯，支持上传和管理大语言模型仓库，包括DeepSeek、Llama 2、Mistral、Qwen等模型。‌

Ollama的主要功能包括：

1. 易于使用‌：提供了一个简单的API，即使是没有经验的用户也可以轻松使用。它还提供了类似ChatGPT的聊天界面，用户无需开发即可直接与模型进行交互。
2. 轻量级‌：代码简洁明了，运行时占用资源少，能够在本地高效地运行，不需要大量的计算资源。
3. 可扩展‌：支持多种模型架构，并可以扩展以支持新的模型。它还支持热加载模型文件，无需重新启动即可切换不同的模型，这使得它非常灵活多变。
4. 预构建模型库‌：提供了一个预构建模型库，可以用于各种任务，如文本生成、翻译、问答等，使得在本地运行大型语言模型变得更加容易和方便。
   Ollama可以在Windows、MacOS和Linux系统上通过命令行界面运行、创建和共享大型语言模型。其设计理念是简化大语言模型的使用流程，兼顾成本效益和隐私保护。 

### 安装和使用Ollama的用例。

[GitHub - ollama/ollama: Get up and running with Llama 3.3, DeepSeek-R1, Phi-4, Gemma 3, Mistral Small 3.1 and other large language models.](https://github.com/ollama/ollama)

参考文档：[Ollama一个简明易用的本地大模型运行框架,只需一条命令即可在本地跑大模型-腾讯云开发者社区-腾讯云](https://cloud.tencent.com/developer/article/2416453)[GitHub - ollama/ollama: Get up and running with Llama 3.3, DeepSeek-R1, Phi-4, Gemma 3, Mistral Small 3.1 and other large language models.](https://github.com/ollama/ollama)

本地安装，使用Ollama的用例。

参考文档：[Ollama一个简明易用的本地大模型运行框架,只需一条命令即可在本地跑大模型-腾讯云开发者社区-腾讯云](https://cloud.tencent.com/developer/article/2416453)

命令：
```
启动
ollama serve
ollama list # 查看本地的模型列表
ollama pull deepseek-r1  # 拉取特定的模型、
ollama run deepseek-r1 # 运行模型

```

可用模型列表：https://ollama.com/search




### 注意和llama的区别？

Llama


## Claude
Claude is a highly performant, trustworthy, and intelligent AI platform built by Anthropic. Claude excels at tasks involving language, reasoning, analysis, coding, and more.




 
 ## vLLM是什么？为什么需要有vLLM？

> https://github.com/vllm-project/vllm
> 

`vLLM` (全称 **Very Large Language Model serving system**) 是一个**开源的大语言模型推理和服务框架**，专注于实现**高吞吐量、低延迟和高硬件利用率**，特别适用于在生产环境中部署和服务大型语言模型。

**简单来说，vLLM 就是一个能让 LLM (如 LLaMA, GPT, Mistral 等) 推理速度更快、同时服务更多用户请求的引擎。**

### vLLM 是什么？（核心是什么）

1.  **定位：** 一个**LLM推理和服务引擎/框架**，不是模型本身。
2.  **核心技术：** 其核心创新在于 **`PagedAttention`** 算法。
    *   灵感来源于操作系统中管理物理内存的**分页机制**。
    *   它将模型运行所需的 `Key` 和 `Value` 张量（在 Transformer 的自注意力机制中占用显存大头）分割成更小的、固定大小的“块”或“页”。
    *   这些页可以**非连续地存储在物理显存**中，通过一个类似于操作系统页表的“块表”来管理逻辑块和物理块之间的映射。
3.  **主要功能：**
    *   **高效推理：** 显著提高 LLM 的推理速度（每秒处理的 token 数）。
    *   **高吞吐服务：** 能同时处理大量并发的用户请求（高吞吐量）。
    *   **优化显存利用：** 大大减少内存碎片，更充分地利用昂贵的 GPU 显存，从而能在同一张 GPU 上运行更大的批次或更大的模型。
    *   **支持主流模型：** 兼容 Hugging Face 格式的众多流行开源模型（LLaMA, Mistral, Qwen, Gemma, Phi 等）。
    *   **易于集成：** 提供与 OpenAI 兼容的 API 服务接口，方便现有应用迁移；支持与 Hugging Face Transformers 集成；也支持作为推理后端集成到更高级的框架中（如 LangChain, LlamaIndex）。
    *   **高级特性：** 支持连续批处理、张量并行、量化、前缀共享等优化技术。

### 为什么需要有 vLLM？（解决的问题）

在 vLLM 出现之前，使用 Hugging Face Transformers 等库直接部署 LLM 服务面临几个关键挑战，而 vLLM 正是为了解决这些痛点而生：

1.  **显存浪费严重 & 内存碎片化：**
    *   **问题：** 传统的动态批处理或简单批处理中，不同请求的序列长度差异巨大。为了一次处理一批请求，系统必须为批次中最长的序列分配足够的显存（用于存储 KV Cache）。这导致大量显存被分配给短序列中未使用的部分，造成**内部碎片**。同时，频繁分配和释放不同大小的显存块会导致**外部碎片**，使得即使总显存足够，也可能找不到连续空间存放新请求。
    *   **vLLM 解法：** `PagedAttention` 将 KV Cache 分页管理。每个请求的 KV Cache 可以分散存储在非连续的显存“页”中。这**极大减少了内存浪费**（因为页大小固定，内部碎片很小），并**几乎消除了外部碎片**（页是固定大小的块，更容易管理），显著**提高了显存利用率**（最高可达传统方法的 3-4 倍）。这意味着**同一张 GPU 上可以运行更大的批次或更大的模型**。

2.  **吞吐量低下：**
    *   **问题：** 显存的低效利用直接限制了可以同时处理的请求数量（批次大小）。低批次大小意味着 GPU 强大的并行计算能力无法被充分利用，导致**总体吞吐量（每秒处理的 token 数或请求数）不高**。
    *   **vLLM 解法：** 通过 `PagedAttention` 大幅提高显存利用率，vLLM 能够实现**非常大的批处理大小**（可能比传统方法大 10 倍甚至更多）。结合**连续批处理**技术（当一个请求生成结束或遇到长序列中的暂停点时，其显存页可以立即释放给新请求使用），vLLM 能持续保持 GPU 满载运行，从而**实现极高的吞吐量**（官方 benchmark 显示比 HuggingFace Transformers 最高可提升 **24 倍**）。

3.  **服务成本高昂：**
    *   **问题：** 低吞吐量意味着为了服务一定量的用户请求，需要部署更多的 GPU 实例。GPU 成本（尤其是高端 GPU）是 LLM 服务的主要开支。
    *   **vLLM 解法：** 通过极高地提升单 GPU 的吞吐量，vLLM **显著降低了服务每个请求所需的计算资源成本**。企业可以用更少的服务器支撑相同的用户流量，或者用相同的服务器支撑更大的用户规模，从而**大幅降低运营成本**。

4.  **高效服务长序列的挑战：**
    *   **问题：** 处理长上下文（如 128K tokens）时，KV Cache 的显存占用变得极其巨大。传统的连续显存分配方式在处理超长序列或混合长短序列时，内存碎片和浪费问题会急剧恶化，甚至导致服务不可行。
    *   **vLLM 解法：** `PagedAttention` 的分页机制天然适合处理超长序列。它只按需分配实际使用的页，并且可以高效地共享不同请求之间共享的前缀序列的 KV Cache（例如系统提示词），进一步节省显存。

**总结为什么需要 vLLM：**

*   **突破显存瓶颈：** 革命性的 `PagedAttention` 解决了 LLM 服务中 KV Cache 显存管理的关键难题（浪费和碎片化）。
*   **释放硬件潜力：** 通过最大化显存利用率和实现超大连续批处理，让昂贵的 GPU 发挥出最大的计算效率。
*   **提升吞吐，降低成本：** 直接带来吞吐量的数量级提升，大幅降低服务 LLM 的每请求成本，使得大规模、高并发、低延迟的 LLM 服务变得经济可行。
*   **推动应用落地：** 为需要高性能、低成本 LLM 服务的应用（聊天机器人、编程助手、内容生成平台等）提供了强大的基础设施支撑。

**简单来说，vLLM 的出现让部署和使用大语言模型变得更快、更便宜、更高效，是推动 LLM 真正走向大规模生产应用的关键技术之一。**


## 什么是LLamaIndex？

> 官方文档：https://docs.llamaindex.ai/en/latest/understanding/putting_it_all_together/q_and_a/#semantic-search
> https://docs.llamaindex.ai/en/latest/examples/

**LlamaIndex**（曾用名 GPT Index）是一个**专为大型语言模型（LLM）设计的开源数据框架**，核心目标是**将你的私有数据或特定领域知识高效地接入 LLM 应用**。它解决了“如何让 LLM 理解和使用它训练数据之外的信息”这一关键问题。

简单来说，LlamaIndex 是 **“LLM 和你数据之间的智能桥梁”**，让 LLM 能够对你的专有文档、数据库、API 等进行问答、分析和推理。

### LlamaIndex 是什么？（核心是什么）

1.  **定位：** 一个 **LLM 的数据接入层/编排框架**，专注于**数据索引、检索和上下文增强**。它不是模型本身，也不是像 vLLM 那样的推理引擎。
2.  **核心技术理念：**
    *   **数据连接器：** 支持从各种来源（PDF, Word, PPT, 网页, 数据库, Notion, Slack, API 等）加载数据。
    *   **数据索引：** 核心功能！将加载的原始数据（尤其是非结构化文本）转换成一种**高度优化、便于 LLM 快速查找和理解的结构化表示形式**。最常见的索引类型是**向量索引**，但也支持列表索引、树形索引、关键词索引、知识图谱索引等。
    *   **查询引擎：** 提供一个简单的接口（`query_engine.query("你的问题")`），接收用户自然语言问题，利用构建好的索引**智能地检索出最相关的上下文信息**，并将其**与用户问题一起组装成提示词（Prompt）** 发送给 LLM，最后将 LLM 生成的答案返回给用户。
    *   **检索增强生成：** LlamaIndex 是 **RAG** 架构的核心实现工具之一。RAG 的核心思想就是在 LLM 生成答案前，先从外部知识源检索相关信息作为上下文。
    *   **代理：** 提供更高级的“代理”功能，可以调用工具（包括查询引擎、API、函数等），进行多步骤推理和决策来回答复杂问题。
3.  **主要功能：**
    *   **构建文档问答系统：** 这是最经典的应用，让 LLM 回答基于你上传文档内容的问题。
    *   **创建聊天机器人（基于知识库）：** 构建拥有特定领域知识的智能客服或助手。
    *   **智能数据分析：** 连接结构化/半结构化数据源（如数据库、CSV、Excel），让 LLM 进行自然语言查询和总结分析。
    *   **自动化文档处理：** 总结报告、提取关键信息、比较文档差异等。
    *   **增强 LLM 的“记忆”：** 为 LLM 提供长期记忆或特定任务背景信息。

### LlamaIndex 解决了什么问题？（痛点）

没有 LlamaIndex（或类似框架）时，开发者想将私有数据接入 LLM 会面临巨大挑战：

1.  **LLM 的“知识冻结”问题：**
    *   **问题：** LLM 只在训练时的数据上拥有知识，无法知晓训练后发生的新事件或你私有的、未公开的数据（如公司内部文档、个人笔记、特定数据库）。直接问 LLM 这些内容，它会“胡编乱造”（幻觉）。
    *   **LlamaIndex 解法：** 提供机制将你的私有数据作为上下文动态注入 LLM 的提示词中，使 LLM 能基于这些最新、特定的信息生成答案。

2.  **上下文窗口长度限制：**
    *   **问题：** LLM 的输入有长度限制（如 GPT-4 Turbo 是128K token）。不可能直接把几十上百页的文档或整个数据库塞进提示词。
    *   **LlamaIndex 解法：** 通过索引（尤其是向量索引），只**检索出与用户问题最相关的文档片段**注入上下文，大大减少所需 token 数，突破原始文档大小的限制。

3.  **信息检索的效率与准确性：**
    *   **问题：** 如何从海量文档中快速准确地找到回答特定问题所需的信息？简单的关键词搜索效果差（语义不匹配），手动查找效率低下。
    *   **LlamaIndex 解法：** 利用**嵌入向量**和**语义搜索**技术。索引将文档内容转换为向量（捕捉语义），查询时也将问题转换为向量，通过计算向量相似度找到**语义上最相关**的段落，显著提升检索质量。

4.  **异构数据源整合困难：**
    *   **问题：** 数据分散在不同格式（文本、表格、PPT）和不同位置（本地文件、云存储、数据库、SaaS 应用）。手动整合费时费力。
    *   **LlamaIndex 解法：** 提供丰富的**数据连接器**，统一从各种来源加载数据，并转换成适合 LLM 处理的格式（通常是文本块）。

5.  **构建 RAG 系统的复杂性：**
    *   **问题：** 实现一个高效、健壮的 RAG 系统需要处理数据加载、分块、嵌入、索引存储、检索、提示词工程、结果集成等多个环节，开发门槛高。
    *   **LlamaIndex 解法：** 提供**开箱即用、高层级的 API**，封装了构建 RAG 的核心步骤（数据加载 -> 索引 -> 查询引擎），极大简化了开发流程。开发者只需关注数据和业务逻辑。

### 为什么需要 LlamaIndex？（价值）

1.  **解锁私有数据价值：** 让 LLM 能够利用你独有的知识资产（文档、数据），产生实际业务价值。没有它，LLM 无法触及这些信息。
2.  **提升答案准确性与可靠性：** 通过提供基于真实文档的上下文，**显著减少 LLM 的“幻觉”**，生成更准确、可验证的答案。答案有据可查（可以追溯到源文档片段）。
3.  **降低成本：** 相比微调大型 LLM（昂贵且复杂），RAG（通过 LlamaIndex 实现）是一种更轻量、更灵活、成本更低的方式来让 LLM 获取新知识。
4.  **保持信息时效性：** 索引可以随时更新，确保 LLM 使用的信息是最新的。无需等待模型重新训练。
5.  **突破模型知识限制：** 让任何 LLM（即使是较小的开源模型）都能获得“无限”的知识扩展能力（受限于你的数据存储和索引能力）。
6.  **大幅降低开发门槛：** 将复杂的 RAG 管道抽象成简单的 API（`from llama_index.core import VectorStoreIndex, SimpleDirectoryReader`），让开发者（甚至非专业 AI 工程师）也能快速构建强大的基于知识的 LLM 应用。
7.  **灵活性：** 支持多种索引类型、多种向量数据库后端、多种 LLM 提供商（OpenAI, Anthropic, Hugging Face, 本地模型等），可定制性强。

**总结为什么需要 LlamaIndex：**

*   **核心驱动力：** LLM 本身无法直接访问你的私有、最新数据，这是其应用的巨大瓶颈。
*   **关键创新：** 提供了标准化、高效的方法（数据加载 -> 智能索引 -> 语义检索 -> 上下文注入）来突破这个瓶颈。
*   **核心价值：** 它让构建 **“懂你数据的智能应用”** 变得**可行、高效且成本可控**。没有 LlamaIndex（或类似框架），开发者需要从零开始实现复杂的 RAG 系统，耗费大量时间和精力。

**简单来说：** 如果你想让 ChatGPT 或任何 LLM 能回答关于你公司内部文档、研究论文、客户数据或个人知识库的问题，而不是只能泛泛而谈，那么 **LlamaIndex 就是实现这一目标的关键工具**。它解决了 LLM 落地应用中最普遍、最关键的“数据接入”难题。